# 2048 RL Dataset Pack - Implementation Guide

## Requirements Recap

**Core Requirements:**
- Dataset contains 1M-1B individual states (steps) from game runs
- PyTorch DataLoader needs to pull **shuffled** batches of 3072 steps efficiently
- 64GB RAM available, dataset size 1-100GB
- Infrequent appends (hourly/daily) of new runs
- Support filtering by various criteria (score, engine, game phase)

**Key Insight:**
The DataLoader with `shuffle=True` means we're doing random access across the entire dataset. Memory-mapped files would page fault constantly. Since we have ample RAM, **just load everything into memory** and serve from there.

## Design Overview

### Core Architecture
```
dataset/
├── pack.dat          # Binary file with all steps + metadata
└── pack.idx          # Optional: Quick lookup index (can rebuild from .dat)
```

### In-Memory Structure
```rust
pub struct DataPack {
    // Primary data - all steps flattened, ignoring run boundaries
    steps: Vec<Step>,

    // Metadata for filtering
    runs: Vec<RunMeta>,

    // Derived indices for fast filtering
    runs_by_score: Vec<(u64, u32)>,     // (score, run_id) sorted
    runs_by_length: Vec<(u32, u32)>,    // (num_steps, run_id) sorted
    step_to_run: Vec<u32>,              // step_idx -> run_id mapping
}

#[repr(C)]  // Ensure stable layout for zero-copy
#[derive(Clone, Copy, Debug)]
pub struct Step {
    board: u64,
    move_dir: u8,        // 0-3 for up/down/left/right
    run_id: u32,         // Which run this came from
    index_in_run: u32,   // Position within that run
    _padding: [u8; 15],  // Pad to 32 bytes for alignment
}

pub struct RunMeta {
    id: u32,
    first_step_idx: u32,  // Index in global steps array
    num_steps: u32,
    max_score: u64,
    highest_tile: u32,
    engine: String,       // Keep as string in memory
    start_time: u64,
    elapsed_s: f32,
}
```

## File Format

### Binary Layout
```
[Header]
magic: [u8; 4] = b"2048"
version: u32 = 1
num_runs: u32
num_steps: u64
metadata_offset: u64  // Where run metadata starts

[Steps Section]
Step_0: [32 bytes]
Step_1: [32 bytes]
...
Step_N: [32 bytes]

[Run Metadata Section]
// For each run:
run_id: u32
first_step_idx: u32
num_steps: u32
max_score: u64
highest_tile: u32
engine_len: u16
engine_bytes: [u8; engine_len]
start_time: u64
elapsed_s: f32

[Trailer]
checksum: u32  // CRC32C of everything above
```

## Implementation Steps

### Step 1: Pack Builder (Rust)
```rust
use rayon::prelude::*;
use std::path::{Path, PathBuf};

pub struct PackBuilder {
    runs: Vec<Run>,
    total_steps: usize,
}

impl PackBuilder {
    pub fn from_directory(dir: &Path) -> Result<Self> {
        // Parallel load all .run files
        let run_files: Vec<PathBuf> = std::fs::read_dir(dir)?
            .filter_map(|e| e.ok())
            .filter(|e| e.path().extension() == Some("run"))
            .map(|e| e.path())
            .collect();

        let runs: Vec<Run> = run_files
            .par_iter()
            .filter_map(|path| parse_run_file(path).ok())
            .collect();

        let total_steps = runs.iter().map(|r| r.meta.steps as usize).sum();

        Ok(PackBuilder { runs, total_steps })
    }

    pub fn build_pack(&self) -> DataPack {
        let mut steps = Vec::with_capacity(self.total_steps);
        let mut runs_meta = Vec::with_capacity(self.runs.len());
        let mut step_to_run = Vec::with_capacity(self.total_steps);

        for (run_id, run) in self.runs.iter().enumerate() {
            let first_step_idx = steps.len() as u32;

            // Convert each state transition to a Step
            for i in 0..run.meta.steps as usize {
                steps.push(Step {
                    board: run.states[i],
                    move_dir: run.moves[i],
                    run_id: run_id as u32,
                    index_in_run: i as u32,
                    _padding: [0; 15],
                });
                step_to_run.push(run_id as u32);
            }

            runs_meta.push(RunMeta {
                id: run_id as u32,
                first_step_idx,
                num_steps: run.meta.steps,
                max_score: run.meta.max_score,
                highest_tile: run.meta.highest_tile,
                engine: run.meta.engine_str.clone().unwrap_or_default(),
                start_time: run.meta.start_unix_s,
                elapsed_s: run.meta.elapsed_s,
            });
        }

        // Build indices for filtering
        let mut runs_by_score: Vec<_> = runs_meta
            .iter()
            .map(|r| (r.max_score, r.id))
            .collect();
        runs_by_score.sort_unstable();

        let mut runs_by_length: Vec<_> = runs_meta
            .iter()
            .map(|r| (r.num_steps, r.id))
            .collect();
        runs_by_length.sort_unstable();

        DataPack {
            steps,
            runs: runs_meta,
            runs_by_score,
            runs_by_length,
            step_to_run,
        }
    }

    pub fn write_to_file(&self, path: &Path) -> Result<()> {
        let pack = self.build_pack();
        pack.save(path)
    }
}
```

### Step 2: DataPack I/O
```rust
impl DataPack {
    pub fn save(&self, path: &Path) -> Result<()> {
        use std::io::Write;
        let mut file = BufWriter::new(File::create(path)?);

        // Write header
        file.write_all(b"2048")?;
        file.write_all(&1u32.to_le_bytes())?;
        file.write_all(&(self.runs.len() as u32).to_le_bytes())?;
        file.write_all(&(self.steps.len() as u64).to_le_bytes())?;

        // Calculate metadata offset
        let metadata_offset = 4 + 4 + 4 + 8 + 8 + (self.steps.len() * 32);
        file.write_all(&(metadata_offset as u64).to_le_bytes())?;

        // Write steps as raw bytes
        let steps_bytes = unsafe {
            std::slice::from_raw_parts(
                self.steps.as_ptr() as *const u8,
                self.steps.len() * std::mem::size_of::<Step>()
            )
        };
        file.write_all(steps_bytes)?;

        // Write run metadata
        for run in &self.runs {
            file.write_all(&run.id.to_le_bytes())?;
            file.write_all(&run.first_step_idx.to_le_bytes())?;
            file.write_all(&run.num_steps.to_le_bytes())?;
            file.write_all(&run.max_score.to_le_bytes())?;
            file.write_all(&run.highest_tile.to_le_bytes())?;
            file.write_all(&(run.engine.len() as u16).to_le_bytes())?;
            file.write_all(run.engine.as_bytes())?;
            file.write_all(&run.start_time.to_le_bytes())?;
            file.write_all(&run.elapsed_s.to_bits().to_le_bytes())?;
        }

        // Write checksum
        file.flush()?;
        let file_contents = std::fs::read(path)?;
        let checksum = crc32c::crc32c(&file_contents[..file_contents.len()-4]);
        file.write_all(&checksum.to_le_bytes())?;

        Ok(())
    }

    pub fn load(path: &Path) -> Result<Self> {
        let data = std::fs::read(path)?;

        // Verify checksum
        let content_len = data.len() - 4;
        let file_checksum = u32::from_le_bytes([
            data[content_len], data[content_len+1],
            data[content_len+2], data[content_len+3]
        ]);
        let calc_checksum = crc32c::crc32c(&data[..content_len]);
        if file_checksum != calc_checksum {
            return Err("Checksum mismatch");
        }

        // Read header
        let mut offset = 0;
        let magic = &data[0..4];
        if magic != b"2048" { return Err("Invalid magic"); }
        offset += 4;

        let version = u32::from_le_bytes([data[offset], data[offset+1], data[offset+2], data[offset+3]]);
        offset += 4;

        let num_runs = u32::from_le_bytes([...]) as usize;
        offset += 4;

        let num_steps = u64::from_le_bytes([...]) as usize;
        offset += 8;

        let metadata_offset = u64::from_le_bytes([...]) as usize;
        offset += 8;

        // Read steps directly into Vec
        let steps_bytes = &data[offset..metadata_offset];
        let steps = unsafe {
            let ptr = steps_bytes.as_ptr() as *const Step;
            let slice = std::slice::from_raw_parts(ptr, num_steps);
            slice.to_vec()
        };

        // Read metadata and rebuild indices
        // ... (parse run metadata from bytes)

        // Rebuild derived indices
        let step_to_run = build_step_to_run_index(&steps);
        let runs_by_score = build_score_index(&runs);
        let runs_by_length = build_length_index(&runs);

        Ok(DataPack { steps, runs, runs_by_score, runs_by_length, step_to_run })
    }
}
```

### Step 3: PyO3 Bindings
```rust
use pyo3::prelude::*;
use numpy::{PyArray1, PyArray2};

#[pyclass]
pub struct Dataset {
    pack: Arc<DataPack>,
    // Optional: filtered view
    valid_indices: Option<Vec<usize>>,
}

#[pymethods]
impl Dataset {
    #[new]
    fn new(path: String) -> PyResult<Self> {
        let pack = DataPack::load(Path::new(&path))
            .map_err(|e| PyErr::new::<pyo3::exceptions::PyIOError, _>(e))?;
        Ok(Dataset {
            pack: Arc::new(pack),
            valid_indices: None,
        })
    }

    fn __len__(&self) -> usize {
        self.valid_indices.as_ref()
            .map(|v| v.len())
            .unwrap_or(self.pack.steps.len())
    }

    fn get_batch(&self, indices: Vec<usize>) -> PyResult<Py<PyArray2<u64>>> {
        // Map through valid_indices if filtered
        let real_indices = if let Some(ref valid) = self.valid_indices {
            indices.iter().map(|&i| valid[i]).collect()
        } else {
            indices
        };

        // Collect boards (you can return more fields if needed)
        let boards: Vec<u64> = real_indices
            .iter()
            .map(|&i| self.pack.steps[i].board)
            .collect();

        Python::with_gil(|py| {
            Ok(PyArray1::from_vec(py, boards).to_owned())
        })
    }

    fn filter_by_score(&self, min_score: u64, max_score: u64) -> Dataset {
        let valid_runs: HashSet<u32> = self.pack.runs
            .iter()
            .filter(|r| r.max_score >= min_score && r.max_score <= max_score)
            .map(|r| r.id)
            .collect();

        let valid_indices: Vec<usize> = self.pack.steps
            .iter()
            .enumerate()
            .filter(|(_, step)| valid_runs.contains(&step.run_id))
            .map(|(i, _)| i)
            .collect();

        Dataset {
            pack: Arc::clone(&self.pack),
            valid_indices: Some(valid_indices),
        }
    }

    // Add more filtering methods as needed
}
```

### Step 4: Incremental Updates
```rust
impl DataPack {
    pub fn append_from_directory(&mut self, dir: &Path) -> Result<()> {
        let new_runs = PackBuilder::from_directory(dir)?;
        let run_id_offset = self.runs.len() as u32;

        for run in new_runs.runs {
            let first_step_idx = self.steps.len() as u32;

            // Add steps
            for i in 0..run.meta.steps as usize {
                self.steps.push(Step {
                    board: run.states[i],
                    move_dir: run.moves[i],
                    run_id: run_id_offset + self.runs.len() as u32,
                    index_in_run: i as u32,
                    _padding: [0; 15],
                });
                self.step_to_run.push(run_id_offset + self.runs.len() as u32);
            }

            // Add run metadata
            self.runs.push(RunMeta { /* ... */ });
        }

        // Rebuild indices
        self.rebuild_indices();
        Ok(())
    }
}
```

## Usage Example
```python
from rl2048_pack import Dataset
import torch
from torch.utils.data import DataLoader

# Load pack
ds = Dataset("data/pack.dat")
print(f"Total steps: {len(ds)}")  # e.g., 50M steps

# Use with PyTorch
class TorchDataset(torch.utils.data.Dataset):
    def __init__(self, pack_ds):
        self.ds = pack_ds

    def __len__(self):
        return len(self.ds)

    def __getitem__(self, idx):
        # DataLoader will batch these
        batch = self.ds.get_batch([idx])
        return torch.from_numpy(batch[0])

loader = DataLoader(
    TorchDataset(ds),
    batch_size=3072,
    shuffle=True,  # Random access - but we're in RAM!
    num_workers=0  # Keep in same process for shared memory
)

# Filtering
high_score_ds = ds.filter_by_score(min_score=50000, max_score=1000000)
```

## Why This Design Works

1. **Optimized for shuffled reads**: Everything in RAM means random access is just pointer arithmetic
2. **Simple implementation**: ~4-6 hours for a grad student familiar with Rust
3. **Fast batch assembly**: Direct memory access, no serialization overhead
4. **Efficient filtering**: Create views without copying data
5. **Incremental updates**: Just append and rebuild indices

## Performance Expectations

- **Loading**: ~5-10 seconds to load 1GB pack file into RAM
- **Batch reads**: <1ms for 3072 random samples (it's just memory copies)
- **Memory usage**: ~32 bytes/step + minimal overhead
- **Filtering**: Near instant (just building index arrays)

## Key Implementation Notes

1. Use `#[repr(C)]` on Step struct for stable memory layout
2. Keep Step at exactly 32 bytes for alignment
3. Use Arc for zero-copy sharing between filtered datasets
4. Don't overthink it - you have RAM, use it!

## Additional Notes

- Have a trait implementation to swap Steps and RunMeta back and forth from the single-run serialization format.
